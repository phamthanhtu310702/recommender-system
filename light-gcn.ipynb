{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement LightGCN\n",
    "This implementation is based on https://arxiv.org/abs/2002.02126 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LightGCN\n",
    "import random\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import model_selection, metrics, preprocessing\n",
    "import copy\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim, Tensor\n",
    "\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "\n",
    "from torch_geometric.utils import structured_negative_sampling\n",
    "from torch_geometric.data import download_url, extract_zip\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.typing import Adj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "- Download MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the dataset\n",
    "# https://grouplens.org/datasets/movielens/\n",
    "# \"Small: 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. Last updated 9/2018\"\n",
    "# url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n",
    "# extract_zip(download_url(url, '.'), '.')\n",
    "\n",
    "movie_path = './ml-latest-small/movies.csv'\n",
    "rating_path = './ml-latest-small/ratings.csv'\n",
    "user_path = './ml-latest-small/users.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n",
      "9724\n",
      "610\n"
     ]
    }
   ],
   "source": [
    "rating_df = pd.read_csv(rating_path)\n",
    "\n",
    "print(rating_df.head())\n",
    "\n",
    "print(len(rating_df['movieId'].unique()))\n",
    "print(len(rating_df['userId'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data before indexing embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[     1      3      6 ... 168250 168252 170875]\n",
      "[  1   1   1 ... 610 610 610]\n"
     ]
    }
   ],
   "source": [
    "# Before preprocessing\n",
    "print(rating_df.movieId.values)\n",
    "print(rating_df.userId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lbl_user = preprocessing.LabelEncoder()\n",
    "lbl_movie = preprocessing.LabelEncoder()\n",
    "\n",
    "rating_df.userId = lbl_user.fit_transform(rating_df.userId.values)\n",
    "rating_df.movieId = lbl_movie.fit_transform(rating_df.movieId.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "609\n",
      "9723\n"
     ]
    }
   ],
   "source": [
    "print(rating_df.userId.max())\n",
    "print(rating_df.movieId.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load edges between users and movies\n",
    "def load_edge_csv(df, \n",
    "                  src_index_col, \n",
    "                  dst_index_col, \n",
    "                  link_index_col, \n",
    "                  rating_threshold=3):\n",
    "    \"\"\"Loads csv containing edges between users and items\n",
    "\n",
    "    Args:\n",
    "        src_index_col (str): column name of users\n",
    "        dst_index_col (str): column name of items\n",
    "        link_index_col (str): column name of user item interaction\n",
    "        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.\n",
    "\n",
    "    Returns:\n",
    "        list of list: edge_index -- 2 by N matrix containing the node ids of N user-item edges\n",
    "        N here is the number of interactions\n",
    "    \"\"\"\n",
    "    edge_index = None\n",
    "\n",
    "    # get user_ids from rating events in the order of occurance\n",
    "    src = [user_id for user_id in  df['userId']]    \n",
    "    # get movie_id from rating events in the order of occurance\n",
    "    dst = [(movie_id) for movie_id in df['movieId']]\n",
    "\n",
    "    # bool type\n",
    "    edge_attr = torch.from_numpy(df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold\n",
    "\n",
    "    edge_index = [[], []]\n",
    "    for i in range(edge_attr.shape[0]):\n",
    "        if edge_attr[i]:\n",
    "            edge_index[0].append(src[i])\n",
    "            edge_index[1].append(dst[i])\n",
    "    return edge_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 x 48580\n"
     ]
    }
   ],
   "source": [
    "edge_index = load_edge_csv(\n",
    "    rating_df,\n",
    "    src_index_col='userId',\n",
    "    dst_index_col='movieId',\n",
    "    link_index_col='rating',\n",
    "    rating_threshold=3.5, \n",
    ")\n",
    "\n",
    "print(f\"{len(edge_index)} x {len(edge_index[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    0,    0,  ...,  609,  609,  609],\n",
      "        [   0,    2,    5,  ..., 9443, 9444, 9445]])\n",
      "torch.Size([2, 48580])\n"
     ]
    }
   ],
   "source": [
    "# Convert to tensor\n",
    "# We use LongTensor here because the .propagate() method in the model needs either LongTensor or  SparseTensor\n",
    "edge_index = torch.LongTensor(edge_index) \n",
    "print(edge_index)\n",
    "print(edge_index.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is the total num_users and num_movies before we apply the rating_threshold\n",
    "num_users = len(rating_df['userId'].unique())\n",
    "num_movies = len(rating_df['movieId'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_interactions = edge_index.shape[1]\n",
    "\n",
    "all_indices = [i for i in range(num_interactions)]\n",
    "\n",
    "train_indices, test_indices = train_test_split(all_indices, \n",
    "                                               test_size=0.2, \n",
    "                                               random_state=1)\n",
    "\n",
    "val_indices, test_indices = train_test_split(test_indices, \n",
    "                                             test_size=0.5, \n",
    "                                             random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edge_index = edge_index[:, train_indices]\n",
    "val_edge_index = edge_index[:, val_indices]\n",
    "test_edge_index = edge_index[:, test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_users 610, num_movies 9724, num_interactions 48580\n",
      "train_edge_index tensor([[ 605,  110,  442,  ...,   65,  161,  427],\n",
      "        [1110, 9619, 1283,  ..., 4640,  443,  827]])\n",
      "10334\n",
      "torch.Size([609])\n",
      "torch.Size([5676])\n"
     ]
    }
   ],
   "source": [
    "print(f\"num_users {num_users}, num_movies {num_movies}, num_interactions {num_interactions}\")\n",
    "print(f\"train_edge_index {train_edge_index}\")\n",
    "print((num_users + num_movies))\n",
    "print(torch.unique(train_edge_index[0]).size())\n",
    "print(torch.unique(train_edge_index[1]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from r_mat (interaction matrix) edge index to adjescency matrix's edge index \n",
    "# so we can feed it to model\n",
    "# the train_edge_index above can not feed to the model\n",
    "\n",
    "def convert_r_mat_edge_index_to_adj_mat_edge_index(input_edge_index):\n",
    "    R = torch.zeros((num_users, num_movies)) # 610x9724\n",
    "    for i in range(len(input_edge_index[0])):\n",
    "        row_idx = input_edge_index[0][i]\n",
    "        col_idx = input_edge_index[1][i]\n",
    "        R[row_idx][col_idx] = 1\n",
    "\n",
    "    R_transpose = torch.transpose(R, 0, 1) #10333 x 10333\n",
    "    adj_mat = torch.zeros((num_users + num_movies , num_users + num_movies))\n",
    "    adj_mat[: num_users, num_users :] = R.clone() #610x9724\n",
    "    adj_mat[num_users :, : num_users] = R_transpose.clone() #9724x610\n",
    "    adj_mat_coo = adj_mat.to_sparse_coo()\n",
    "    adj_mat_coo = adj_mat_coo.indices()\n",
    "    return adj_mat_coo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_adj_mat_edge_index_to_r_mat_edge_index(input_edge_index):\n",
    "    sparse_input_edge_index = SparseTensor(row=input_edge_index[0], \n",
    "                                           col=input_edge_index[1], \n",
    "                                           sparse_sizes=((num_users + num_movies), num_users + num_movies))\n",
    "    adj_mat = sparse_input_edge_index.to_dense()\n",
    "    interact_mat = adj_mat[: num_users, num_users :]\n",
    "    r_mat_edge_index = interact_mat.to_sparse_coo().indices()\n",
    "    return r_mat_edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(train_edge_index)\n",
    "val_edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(val_edge_index)\n",
    "test_edge_index = convert_r_mat_edge_index_to_adj_mat_edge_index(test_edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     0,     0,  ..., 10326, 10327, 10333],\n",
      "        [  610,   612,   653,  ...,   183,   183,   330]])\n",
      "torch.Size([2, 77728])\n",
      "tensor([[    0,     0,     0,  ..., 10226, 10236, 10240],\n",
      "        [  615,   794,  2010,  ...,   317,   204,   413]])\n",
      "torch.Size([2, 9716])\n",
      "tensor([[    0,     0,     0,  ..., 10301, 10302, 10329],\n",
      "        [  811,  1086,  1095,  ...,   585,   585,   183]])\n",
      "torch.Size([2, 9716])\n"
     ]
    }
   ],
   "source": [
    "print(train_edge_index)\n",
    "print(train_edge_index.size())\n",
    "print(val_edge_index)\n",
    "print(val_edge_index.size())\n",
    "print(test_edge_index)\n",
    "print(test_edge_index.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for training and compute BPR loss\n",
    "# since this is a self-supervised learning, we are relying on the graph structure itself and \n",
    "# we don't have label other than the graph structure so we need to the folloing function\n",
    "# which random samples a mini-batch of positive and negative samples\n",
    "\n",
    "# In link prediction tasks, \n",
    "# it is usual to treat existing edges in the graph as positive examples, \n",
    "# and non-existent edges as negative examples.\n",
    "# i.e. in training/prediction you feed the network a subset of all edges in the complete graph, \n",
    "# and the associated targets are \"this is a real edge\" (positive) and \"this is not a real edge\" (negative).\n",
    "\n",
    "def sample_mini_batch(batch_size, edge_index):\n",
    "    \"\"\"Randomly samples indices of a minibatch given an adjacency matrix\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): minibatch size\n",
    "        edge_index (torch.Tensor): 2 by N list of edges\n",
    "\n",
    "    Returns:\n",
    "        tuple: user indices, positive item indices, negative item indices\n",
    "    \"\"\"\n",
    "    # structured_negative_sampling is a pyG library\n",
    "    # Samples a negative edge :obj:`(i,k)` for every positive edge\n",
    "    # :obj:`(i,j)` in the graph given by :attr:`edge_index`, and returns it as a\n",
    "    # tuple of the form :obj:`(i,j,k)`.\n",
    "    #\n",
    "    #         >>> edge_index = torch.as_tensor([[0, 0, 1, 2],\n",
    "    #         ...                               [0, 1, 2, 3]])\n",
    "    #         >>> structured_negative_sampling(edge_index)\n",
    "    #         (tensor([0, 0, 1, 2]), tensor([0, 1, 2, 3]), tensor([2, 3, 0, 2]))\n",
    "    edges = structured_negative_sampling(edge_index)\n",
    "    \n",
    "    # 3 x edge_index_len\n",
    "    edges = torch.stack(edges, dim=0)\n",
    "    \n",
    "    # here is whhen we actually perform the batch sampe\n",
    "    # Return a k sized list of population elements chosen with replacement.\n",
    "    indices = random.choices([i for i in range(edges[0].shape[0])], k=batch_size)\n",
    "    \n",
    "    batch = edges[:, indices]\n",
    "    \n",
    "    user_indices, pos_item_indices, neg_item_indices = batch[0], batch[1], batch[2]\n",
    "    return user_indices, pos_item_indices, neg_item_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = 3    \n",
    "model = LightGCN(num_users=num_users, \n",
    "                 num_items=num_movies, \n",
    "                 K=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "- Bayesian Personalized Ranking (BPR) loss\n",
    "\\begin{equation}\n",
    "L_{BPR} = -\\sum_{u = 1}^M \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln{\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})} + \\lambda ||E^{(0)}||^2 \n",
    "\\end{equation}\n",
    "\n",
    "$\\hat{y}_{u}$: predicted score of a positive sample\n",
    "\n",
    "$\\hat{y}_{uj}$: predicted score of a negative sample\n",
    "\n",
    "$\\lambda$: hyperparameter which controls the L2 regularization strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "Precision and Recall\n",
    "\n",
    "Precision attempts to answer the following question:\n",
    "\n",
    "- What proportion of positive identifications was actually correct?\n",
    "\n",
    "Recall attempts to answer the following question:\n",
    "\n",
    "- What proportion of actual positives was identified correctly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define contants\n",
    "ITERATIONS = 1000\n",
    "EPOCHS = 10\n",
    "# ITERATIONS = 500\n",
    "BATCH_SIZE = 1024\n",
    "LR = 1e-3\n",
    "ITERS_PER_EVAL = 200\n",
    "ITERS_PER_LR_DECAY = 200\n",
    "K = 20\n",
    "LAMBDA = 1e-6\n",
    "# LAMBDA = 1/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu.\n"
     ]
    }
   ],
   "source": [
    "# setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device {device}.\")\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n",
    "\n",
    "edge_index = edge_index.to(device)\n",
    "train_edge_index = train_edge_index.to(device)\n",
    "val_edge_index = val_edge_index.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loss import get_embs_for_bpr\n",
    "from evaluation import evaluation_model\n",
    "from loss import bpr_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "718aae80e1d84fae8b4bf1178746150e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iteration 0/1000] train_loss: -2.04517, val_loss: -1.61007, val_recall@20: 0.04188, val_precision@20: 0.01347, val_ndcg@20: 0.02752\n",
      "[Iteration 200/1000] train_loss: -20.45242, val_loss: -14.33949, val_recall@20: 0.08122, val_precision@20: 0.02505, val_ndcg@20: 0.05661\n",
      "[Iteration 400/1000] train_loss: -52.83073, val_loss: -36.7243, val_recall@20: 0.10132, val_precision@20: 0.03083, val_ndcg@20: 0.06916\n",
      "[Iteration 600/1000] train_loss: -86.71727, val_loss: -65.34219, val_recall@20: 0.10388, val_precision@20: 0.03264, val_ndcg@20: 0.07249\n",
      "[Iteration 800/1000] train_loss: -133.3737, val_loss: -99.84721, val_recall@20: 0.11645, val_precision@20: 0.03508, val_ndcg@20: 0.08082\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_recall_at_ks = []\n",
    "\n",
    "for iter in tqdm(range(ITERATIONS)):\n",
    "    # forward propagation  \n",
    "    users_emb_final, users_emb_0,  pos_items_emb_final, pos_items_emb_0, neg_items_emb_final, neg_items_emb_0 \\\n",
    "                = get_embs_for_bpr(model, train_edge_index,BATCH_SIZE, device, num_users,num_movies)\n",
    "    \n",
    "    # loss computation\n",
    "    train_loss = bpr_loss(users_emb_final, \n",
    "                          users_emb_0, \n",
    "                          pos_items_emb_final,\n",
    "                          pos_items_emb_0, \n",
    "                          neg_items_emb_final, \n",
    "                          neg_items_emb_0, \n",
    "                          LAMBDA)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # validation set\n",
    "    if iter % ITERS_PER_EVAL == 0:\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_loss, recall, precision, ndcg = evaluation_model(model, \n",
    "                                                           val_edge_index, \n",
    "                                                           [train_edge_index], \n",
    "                                                           K, \n",
    "                                                           LAMBDA,\n",
    "                                                           num_users,\n",
    "                                                           num_movies\n",
    "                                                          )\n",
    "\n",
    "            print(f\"[Iteration {iter}/{ITERATIONS}] train_loss: {round(train_loss.item(), 5)}, val_loss: {round(val_loss, 5)}, val_recall@{K}: {round(recall, 5)}, val_precision@{K}: {round(precision, 5)}, val_ndcg@{K}: {round(ndcg, 5)}\")\n",
    "\n",
    "            train_losses.append(train_loss.item())\n",
    "            val_losses.append(val_loss)\n",
    "            val_recall_at_ks.append(round(recall, 5))\n",
    "        model.train()\n",
    "\n",
    "    if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('pyg-m1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b850cb4b155d96b37fc1920e6e804982be6bf4912de396685edcf3fcf327da5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
